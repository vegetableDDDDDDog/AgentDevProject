"""
RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) Agent
æ”¯æŒåŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”ç³»ç»Ÿ
"""

import os
from typing import List

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import (
    TextLoader,
    DirectoryLoader,
    PyPDFLoader,
)
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# é…ç½®
PERSIST_DIRECTORY = os.path.join(PROJECT_ROOT, "data", "chroma_db")  # å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
CHUNK_SIZE = 500  # æ–‡æ¡£åˆ†å—å¤§å°
CHUNK_OVERLAP = 50  # åˆ†å—é‡å å¤§å°
TOP_K = 3  # æ£€ç´¢æœ€ç›¸å…³çš„ K ä¸ªæ–‡æ¡£å—


class RAGAgent:
    """RAG çŸ¥è¯†åº“é—®ç­” Agent"""

    def __init__(
        self,
        model_name: str = None,
        persist_directory: str = PERSIST_DIRECTORY,
    ):
        """åˆå§‹åŒ– RAG Agent

        Args:
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            persist_directory: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
        """
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.api_base = os.getenv("OPENAI_API_BASE")
        self.model_name = model_name or os.getenv("OPENAI_MODEL", "glm-4")
        self.embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-v3")

        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url=self.api_base,
            temperature=0.7,
        )

        # åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆæ™ºè°± AI æ”¯æŒçš„ embedding æ¨¡å‹ï¼‰
        self.embeddings = OpenAIEmbeddings(
            model=self.embedding_model,
            api_key=self.api_key,
            base_url=self.api_base,
        )

        # æŒä¹…åŒ–ç›®å½•
        self.persist_directory = persist_directory

        # å‘é‡æ•°æ®åº“ï¼ˆæ‡’åŠ è½½ï¼‰
        self.vectorstore = None

        # RAG æç¤ºè¯æ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæä¾›çš„çŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

è¯·æ ¹æ®ä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†è¡¥å……è¯´æ˜ï¼Œä½†è¦æ˜ç¡®åŒºåˆ†
3. å›ç­”è¦å‡†ç¡®ã€æ¸…æ™°ã€æœ‰æ¡ç†"""),
            ("human", "{question}"),
        ])

        # åˆ›å»º RAG Chain
        self.rag_chain = (
            {
                "context": self._retrieve_context,
                "question": lambda x: x["question"],
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _retrieve_context(self, inputs: dict) -> str:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ä¸Šä¸‹æ–‡

        Args:
            inputs: åŒ…å« question çš„å­—å…¸

        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥å­—ç¬¦ä¸²
        """
        if self.vectorstore is None:
            return "ï¼ˆçŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ–‡æ¡£ï¼‰"

        question = inputs["question"]

        # ç›¸ä¼¼åº¦æœç´¢
        docs = self.vectorstore.similarity_search(question, k=TOP_K)

        # æ‹¼æ¥æ–‡æ¡£å†…å®¹
        context_parts = []
        for i, doc in enumerate(docs, 1):
            context_parts.append(f"[æ–‡æ¡£ç‰‡æ®µ {i}]\n{doc.page_content}\n")

        return "\n".join(context_parts) if context_parts else "ï¼ˆæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼‰"

    def load_documents(self, path: str, glob: str = "**/*.txt") -> int:
        """ä»ç›®å½•åŠ è½½æ–‡æ¡£

        Args:
            path: æ–‡æ¡£ç›®å½•è·¯å¾„
            glob: æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œæ”¯æŒ *.txt, *.md, *.pdf ç­‰

        Returns:
            åŠ è½½çš„æ–‡æ¡£æ•°é‡
        """
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ–‡æ¡£: {path}")

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
        if glob.endswith(".pdf"):
            loader = DirectoryLoader(
                path,
                glob=glob,
                loader_cls=PyPDFLoader,
                show_progress=True,
            )
        else:
            loader = DirectoryLoader(
                path,
                glob=glob,
                loader_cls=TextLoader,
                loader_kwargs={"autodetect_encoding": True},
                show_progress=True,
            )

        # åŠ è½½æ–‡æ¡£
        documents = loader.load()
        print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

        # åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""],
        )

        print("âœ‚ï¸  æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        splits = text_splitter.split_documents(documents)
        print(f"âœ… åˆ†å‰²æˆ {len(splits)} ä¸ªæ–‡æœ¬å—")

        # åˆ›å»ºæˆ–æ›´æ–°å‘é‡æ•°æ®åº“
        print(f"ğŸ’¾ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")

        if self.vectorstore is None:
            # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            self.vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=self.persist_directory,
            )
        else:
            # æ·»åŠ åˆ°ç°æœ‰å‘é‡æ•°æ®åº“
            self.vectorstore.add_documents(splits)

        print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {self.persist_directory}")
        return len(documents)

    def load_text(self, text: str, metadata: dict = None) -> int:
        """ç›´æ¥åŠ è½½æ–‡æœ¬å†…å®¹

        Args:
            text: æ–‡æœ¬å†…å®¹
            metadata: æ–‡æ¡£å…ƒæ•°æ®

        Returns:
            æ–‡æœ¬å—æ•°é‡
        """
        # åˆ›å»ºæ–‡æ¡£
        doc = Document(page_content=text, metadata=metadata or {})

        # åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
        )

        splits = text_splitter.split_documents([doc])

        # åˆ›å»ºæˆ–æ›´æ–°å‘é‡æ•°æ®åº“
        if self.vectorstore is None:
            self.vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=self.persist_directory,
            )
        else:
            self.vectorstore.add_documents(splits)

        return len(splits)

    def load_existing_vectorstore(self):
        """åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“"""
        if os.path.exists(self.persist_directory):
            print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {self.persist_directory}")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings,
            )
            print(f"âœ… å‘é‡æ•°æ®åº“å·²åŠ è½½")
            return True
        else:
            print(f"âŒ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {self.persist_directory}")
            print(f"ğŸ’¡ æç¤ºï¼šè¯·å…ˆä½¿ç”¨ load_documents() æˆ– load_text() åŠ è½½æ–‡æ¡£")
            return False

    def query(self, question: str, stream: bool = False) -> str:
        """æŸ¥è¯¢çŸ¥è¯†åº“

        Args:
            question: ç”¨æˆ·é—®é¢˜
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º

        Returns:
            AI å›ç­”
        """
        if self.vectorstore is None:
            # å°è¯•åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
            if not self.load_existing_vectorstore():
                return "âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–ã€‚è¯·å…ˆä½¿ç”¨ load_documents() æˆ– load_text() åŠ è½½æ–‡æ¡£ã€‚"

        print(f"\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
        print(f"ğŸ“š æ£€ç´¢ç›¸å…³æ–‡æ¡£...")

        if stream:
            # æµå¼è¾“å‡º
            print("\nğŸ’¬ å›ç­”ï¼š")
            for chunk in self.rag_chain.stream({"question": question}):
                print(chunk, end="", flush=True)
            print()  # æ¢è¡Œ
            return ""
        else:
            # ä¸€æ¬¡æ€§è¾“å‡º
            answer = self.rag_chain.invoke({"question": question})
            return answer

    def chat(self):
        """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
        print("=" * 50)
        print("ğŸ¤– RAG çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
        print("=" * 50)

        # å°è¯•åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        if self.vectorstore is None:
            self.load_existing_vectorstore()

        print("\nğŸ’¡ æç¤ºï¼š")
        print("  - ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢")
        print("  - è¾“å…¥ 'load <è·¯å¾„>' åŠ è½½æ–‡æ¡£ç›®å½•")
        print("  - è¾“å…¥ 'add <æ–‡æœ¬>' ç›´æ¥æ·»åŠ æ–‡æœ¬")
        print("  - è¾“å…¥ 'clear' æ¸…é™¤å½“å‰ä¼šè¯")
        print("  - è¾“å…¥ 'status' æŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€")
        print("  - è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")

        while True:
            try:
                user_input = input("\nğŸ‘¤ ä½ : ").strip()

                if not user_input:
                    continue

                # å¤„ç†å‘½ä»¤
                if user_input.lower() in ["exit", "quit", "q"]:
                    print("\nğŸ‘‹ å†è§ï¼")
                    break

                elif user_input.lower() in ["clear", "cls"]:
                    print("\nâœ… å·²æ¸…é™¤å½“å‰ä¼šè¯")
                    continue

                elif user_input.lower() in ["status", "info"]:
                    self._print_status()
                    continue

                elif user_input.lower().startswith("load "):
                    # åŠ è½½æ–‡æ¡£ç›®å½•
                    path = user_input[5:].strip()
                    if os.path.isdir(path):
                        self.load_documents(path)
                    else:
                        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {path}")
                    continue

                elif user_input.lower().startswith("add "):
                    # æ·»åŠ æ–‡æœ¬
                    text = user_input[4:].strip()
                    chunks = self.load_text(text, metadata={"source": "user_input"})
                    print(f"âœ… å·²æ·»åŠ  {chunks} ä¸ªæ–‡æœ¬å—")
                    continue

                # æ™®é€šæŸ¥è¯¢
                answer = self.query(user_input, stream=True)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")

    def _print_status(self):
        """æ‰“å°çŸ¥è¯†åº“çŠ¶æ€"""
        print("\nğŸ“Š çŸ¥è¯†åº“çŠ¶æ€ï¼š")
        print(f"  æ¨¡å‹: {self.model_name}")
        print(f"  Embedding: {self.embedding_model}")

        if self.vectorstore is None:
            print(f"  çŠ¶æ€: æœªåˆå§‹åŒ–")
            print(f"  æŒä¹…åŒ–ç›®å½•: {self.persist_directory}")
        else:
            # è·å–å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡
            collection = self.vectorstore._collection
            count = collection.count()
            print(f"  çŠ¶æ€: å·²åŠ è½½")
            print(f"  æ–‡æœ¬å—æ•°é‡: {count}")
            print(f"  æŒä¹…åŒ–ç›®å½•: {self.persist_directory}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="RAG çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    parser.add_argument(
        "--load", "-l",
        type=str,
        help="åŠ è½½æ–‡æ¡£ç›®å½•"
    )
    parser.add_argument(
        "--query", "-q",
        type=str,
        help="æŸ¥è¯¢é—®é¢˜ï¼ˆå•æ¬¡æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--interactive", "-i",
        action="store_true",
        help="è¿›å…¥äº¤äº’æ¨¡å¼"
    )

    args = parser.parse_args()

    # åˆ›å»º RAG Agent
    agent = RAGAgent()

    # åŠ è½½æ–‡æ¡£
    if args.load:
        agent.load_documents(args.load)

    # å•æ¬¡æŸ¥è¯¢
    if args.query:
        answer = agent.query(args.query)
        print(f"\nğŸ’¬ å›ç­”ï¼š\n{answer}")
        return

    # äº¤äº’æ¨¡å¼æˆ–é»˜è®¤æ¨¡å¼
    if args.interactive or not (args.load or args.query):
        agent.chat()


if __name__ == "__main__":
    main()
