# RAG çŸ¥è¯†åº“ä½¿ç”¨æŠ€å·§æŒ‡å—

> ğŸ“š åŸºäº `rag_agent.py` çš„å®æˆ˜ç»éªŒæ€»ç»“
> ğŸ¯ å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹å¹¶ä¼˜åŒ– RAG åº”ç”¨

---

## ğŸ“– ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
3. [ä½¿ç”¨æŠ€å·§](#ä½¿ç”¨æŠ€å·§)
4. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
5. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
7. [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç”¨æ³•

```python
from agents.rag_agent import RAGAgent

# åˆ›å»º Agent
agent = RAGAgent()

# åŠ è½½æ–‡æ¡£
agent.load_documents("./knowledge_base")

# æŸ¥è¯¢
answer = agent.query("ä½ çš„é—®é¢˜")
```

### äº¤äº’æ¨¡å¼

```bash
# ç›´æ¥è¿è¡Œï¼ˆè‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“ï¼‰
python agents/rag_agent.py

# åŠ è½½æ–°æ–‡æ¡£
python agents/rag_agent.py --load ./docs

# å•æ¬¡æŸ¥è¯¢
python agents/rag_agent.py --query "é—®é¢˜"
```

---

## ğŸ§  æ ¸å¿ƒæ¦‚å¿µ

### RAG å·¥ä½œæµç¨‹

```
ç”¨æˆ·æé—®
   â†“
å‘é‡åŒ–é—®é¢˜
   â†“
ç›¸ä¼¼åº¦æ£€ç´¢ (Top-K)
   â†“
æ‹¼æ¥ä¸Šä¸‹æ–‡
   â†“
LLM ç”Ÿæˆå›ç­”
```

### å…³é”®å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|--------|------|----------|
| `CHUNK_SIZE` | 500 | æ–‡æ¡£åˆ†å—å¤§å° | å°æ–‡æ¡£â†’300ï¼Œå¤§æ–‡æ¡£â†’800 |
| `CHUNK_OVERLAP` | 50 | åˆ†å—é‡å å¤§å° | ä¿æŒ 10-15% çš„å—å¤§å° |
| `TOP_K` | 3 | æ£€ç´¢æ–‡æ¡£æ•°é‡ | 3-5 ä¸ªæœ€ä½³ï¼Œè¿‡å¤šä¼šå¢åŠ  token |
| `embedding_model` | embedding-3 | å‘é‡åŒ–æ¨¡å‹ | æ™ºè°± AI ä½¿ç”¨ `embedding-3` |

---

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. æ–‡æ¡£å‡†å¤‡æŠ€å·§

#### âœ… å¥½çš„æ–‡æ¡£ç»“æ„

```
knowledge_base/
â”œâ”€â”€ python_basics/
â”‚   â”œâ”€â”€ 01_intro.txt      # æœ‰ç¼–å·ï¼Œé€»è¾‘æ¸…æ™°
â”‚   â”œâ”€â”€ 02_syntax.txt
â”‚   â””â”€â”€ 03_oop.txt
â”œâ”€â”€ langchain/
â”‚   â”œâ”€â”€ chains.md
â”‚   â””â”€â”€ agents.md
â””â”€â”€ faq/
    â””â”€â”€ common_questions.txt
```

#### âŒ é¿å…çš„åšæ³•

- æ–‡ä»¶åå…¨æ˜¯ä¸­æ–‡æˆ–ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½ç¼–ç é—®é¢˜ï¼‰
- å•ä¸ªæ–‡ä»¶è¿‡å¤§ï¼ˆ>10MBï¼‰ï¼Œå»ºè®®æ‹†åˆ†
- æ–‡æ¡£æ ¼å¼æ··ä¹±ï¼ˆæ··æ‚ Markdownã€çº¯æ–‡æœ¬ã€HTMLï¼‰

### 2. æ–‡æœ¬åˆ†å‰²ä¼˜åŒ–

#### é’ˆå¯¹ä¸åŒå†…å®¹è°ƒæ•´åˆ†å‰²å™¨

```python
# ä»£ç æ–‡æ¡£ - ä½¿ç”¨æ›´å°çš„å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=30,
    separators=["\n\n", "\n", "    ", " "],  # ä¿ç•™ä»£ç ç¼©è¿›
)

# é•¿ç¯‡æ–‡æ¡£ - ä½¿ç”¨æ›´å¤§çš„å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=80,
    separators=["\n\n##", "\n\n", "\n", "ã€‚", " "],
)

# FAQ æ–‡æ¡£ - æŒ‰é—®é¢˜åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=0,  # FAQ ä¸éœ€è¦é‡å 
    separators=["\né—®:", "\nQ:", "\n---", "\n\n"],
)
```

### 3. å…ƒæ•°æ®å¢å¼º

ä¸ºæ–‡æ¡£æ·»åŠ ç»“æ„åŒ–å…ƒæ•°æ®ï¼Œæé«˜æ£€ç´¢ç²¾åº¦ï¼š

```python
from langchain_core.documents import Document

doc = Document(
    page_content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€...",
    metadata={
        "source": "python_intro.txt",
        "category": "ç¼–ç¨‹è¯­è¨€",
        "difficulty": "å…¥é—¨",
        "tags": ["python", "åŸºç¡€"],
        "date": "2024-01-01",
    }
)
```

### 4. æ£€ç´¢ç­–ç•¥

#### å•è½®æ£€ç´¢ï¼ˆé»˜è®¤ï¼‰

```python
docs = vectorstore.similarity_search("é—®é¢˜", k=3)
```

#### å¸¦åˆ†æ•°çš„æ£€ç´¢

```python
docs_with_scores = vectorstore.similarity_search_with_score("é—®é¢˜", k=3)
for doc, score in docs_with_scores:
    print(f"ç›¸ä¼¼åº¦: {score:.4f} | {doc.page_content[:50]}...")
```

#### å¤šæŸ¥è¯¢æ£€ç´¢ï¼ˆæé«˜å¬å›ç‡ï¼‰

```python
# å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºå¤šä¸ªå­é—®é¢˜
questions = [
    "Python çš„æ•°æ®ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
    "Python å˜é‡å¦‚ä½•å£°æ˜ï¼Ÿ",
]

all_docs = []
for q in questions:
    docs = vectorstore.similarity_search(q, k=2)
    all_docs.extend(docs)

# å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
unique_docs = list({doc.page_content: doc for doc in all_docs}.values())
```

### 5. æç¤ºè¯å·¥ç¨‹

#### åŸºç¡€æç¤ºè¯æ¨¡æ¿

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯çŸ¥è¯†åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹å†…å®¹å›ç­”ï¼š

{context}

å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ï¼Œä¸è¦ç¼–é€ ã€‚"""),
    ("human", "{question}"),
])
```

#### è¿›é˜¶æç¤ºè¯æŠ€å·§

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ã€‚

**å‚è€ƒæ–‡æ¡£ï¼š**
{context}

**å›ç­”è¦æ±‚ï¼š**
1. å¿…é¡»åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”
2. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œå…ˆè¯´æ˜æ–‡æ¡£å†…å®¹ï¼Œå†è¡¥å……é€šç”¨çŸ¥è¯†
3. å¼•ç”¨å…·ä½“æ–‡æ¡£æ¥æºï¼ˆå¦‚ [æ–‡æ¡£ç‰‡æ®µ 1]ï¼‰
4. å¯¹äºæŠ€æœ¯é—®é¢˜ï¼Œæä¾›ä»£ç ç¤ºä¾‹
5. ä½¿ç”¨åˆ†ç‚¹é™ˆè¿°ï¼Œç»“æ„æ¸…æ™°

**å½“å‰æ—¶é—´ï¼š**{current_time}"""),
    ("human", "{question}"),
])
```

---

## ğŸ† æœ€ä½³å®è·µ

### 1. çŸ¥è¯†åº“ç»„ç»‡

```
project_knowledge/
â”œâ”€â”€ product_docs/          # äº§å“æ–‡æ¡£
â”œâ”€â”€ api_reference/         # API å‚è€ƒ
â”œâ”€â”€ troubleshooting/       # æ•…éšœæ’é™¤
â”œâ”€â”€ tutorials/             # æ•™ç¨‹
â””â”€â”€ faq/                   # å¸¸è§é—®é¢˜
```

**å¥½å¤„ï¼š**
- ä¾¿äºç®¡ç†å’Œæ›´æ–°
- å¯ä»¥é’ˆå¯¹ä¸åŒç±»åˆ«è°ƒæ•´æ£€ç´¢ç­–ç•¥
- æ”¯æŒå¢é‡æ›´æ–°ï¼ˆåªéœ€é‡æ–°åŠ è½½ä¿®æ”¹çš„ç›®å½•ï¼‰

### 2. å¢é‡æ›´æ–°ç­–ç•¥

```python
# ä¸è¦æ¯æ¬¡éƒ½é‡å»ºæ•´ä¸ªå‘é‡åº“

# âŒ ä½æ•ˆåšæ³•
agent.load_documents("./knowledge_base")  # é‡æ–°åŠ è½½æ‰€æœ‰æ–‡æ¡£

# âœ… é«˜æ•ˆåšæ³•
# åªåŠ è½½æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡æ¡£
agent.load_documents("./knowledge_base/new_docs")
agent.vectorstore.persist()  # æŒä¹…åŒ–
```

### 3. æ··åˆæ£€ç´¢ç­–ç•¥

```python
# ç»“åˆå…³é”®è¯æ£€ç´¢å’Œè¯­ä¹‰æ£€ç´¢
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# å…³é”®è¯æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_documents(splits)

# è¯­ä¹‰æ£€ç´¢å™¨
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# æ··åˆæ£€ç´¢å™¨
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7],  # è°ƒæ•´æƒé‡
)
```

### 4. ä¸Šä¸‹æ–‡çª—å£ç®¡ç†

```python
# é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´ token è¶…é™

def smart_retrieval(question, max_tokens=2000):
    docs = vectorstore.similarity_search(question, k=5)

    selected_docs = []
    total_tokens = 0

    for doc in docs:
        doc_tokens = len(doc.page_content)
        if total_tokens + doc_tokens <= max_tokens:
            selected_docs.append(doc)
            total_tokens += doc_tokens
        else:
            break

    return selected_docs
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜ Embedding ç»“æœ

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embed(text):
    return embeddings.embed_query(text)
```

### 2. æ‰¹é‡å¤„ç†

```python
# âœ… æ‰¹é‡ Embeddingï¼ˆæ›´å¿«ï¼‰
embeddings.embed_documents([
    "æ–‡æœ¬1",
    "æ–‡æœ¬2",
    "æ–‡æœ¬3",
])

# âŒ é€ä¸ª Embeddingï¼ˆæ…¢ï¼‰
for text in texts:
    embeddings.embed_query(text)
```

### 3. å¹¶è¡ŒåŠ è½½

```python
from concurrent.futures import ThreadPoolExecutor

def load_multiple_directories(paths):
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [
            executor.submit(agent.load_documents, path)
            for path in paths
        ]
        results = [f.result() for f in futures]
    return sum(results)
```

### 4. ç¡¬ä»¶åŠ é€Ÿ

```bash
# å®‰è£… GPU ç‰ˆæœ¬çš„ä¾èµ–ï¼ˆå¦‚æœæœ‰ GPUï¼‰
pip install faiss-gpu  # ä½¿ç”¨ Faiss-GPU åŠ é€Ÿæ£€ç´¢
```

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: æ£€ç´¢ç»“æœä¸ç›¸å…³ï¼Ÿ

**åŸå› ï¼š**
- æ–‡æ¡£åˆ†å—ä¸åˆç†ï¼ˆä¸Šä¸‹æ–‡è¢«åˆ‡æ–­ï¼‰
- é—®é¢˜è¡¨è¿°ä¸æ¸…æ¥š
- ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®ä¸å½“

**è§£å†³ï¼š**
```python
# 1. è°ƒæ•´åˆ†å—å¤§å°
CHUNK_SIZE = 300  # å‡å°å—å¤§å°ï¼Œæé«˜ç²¾åº¦

# 2. å¢åŠ é‡å 
CHUNK_OVERLAP = 100  # å¢åŠ ä¸Šä¸‹æ–‡è¿è´¯æ€§

# 3. ä½¿ç”¨æ›´å¥½çš„åˆ†å‰²ç¬¦
separators=["\n\n##", "\n\n", "\n", "ã€‚", " ", ""]
```

### Q2: å›ç­”å¤ªç®€ç•¥æˆ–å¤ªå†—é•¿ï¼Ÿ

**è°ƒæ•´æç¤ºè¯ï¼š**
```python
# ç®€ç•¥å›ç­”
prompt = """ç”¨ä¸€å¥è¯ç®€è¦å›ç­”ï¼š{context}\n\né—®é¢˜ï¼š{question}"""

# è¯¦ç»†å›ç­”
prompt = """è¯·è¯¦ç»†å›ç­”ï¼Œæä¾›å®Œæ•´ä¿¡æ¯å’Œç¤ºä¾‹ï¼š{context}\n\né—®é¢˜ï¼š{question}"""
```

### Q3: å‘é‡æ•°æ®åº“å ç”¨ç©ºé—´å¤ªå¤§ï¼Ÿ

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
```python
# 1. å‡å°å‘é‡ç»´åº¦
embeddings = OpenAIEmbeddings(model="embedding-2")  # æ›´å°

# 2. å®šæœŸæ¸…ç†
if os.path.exists("./chroma_db"):
    shutil.rmtree("./chroma_db")

# 3. ä½¿ç”¨æ›´é«˜æ•ˆçš„å‘é‡å­˜å‚¨
pip install faiss-cpu  # æ¯” Chroma æ›´èŠ‚çœç©ºé—´
```

### Q4: API è°ƒç”¨è´¹ç”¨å¤ªé«˜ï¼Ÿ

**çœé’±æŠ€å·§ï¼š**
```python
# 1. å‡å°‘æ£€ç´¢æ•°é‡
TOP_K = 2  # ä» 3 é™åˆ° 2

# 2. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
llm = ChatOpenAI(model="glm-3-turbo")  # æ¯” glm-4 ä¾¿å®œ

# 3. ç¼“å­˜å¸¸è§é—®é¢˜ç­”æ¡ˆ
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_query(question):
    return agent.query(question)
```

---

## ğŸ“š å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šæŠ€æœ¯æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
# tech_support_agent.py
from agents.rag_agent import RAGAgent

agent = RAGAgent()
agent.load_documents("./tech_docs")

def tech_support_query(question):
    # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    enhanced_question = f"""
    ä½œä¸ºæŠ€æœ¯æ”¯æŒå·¥ç¨‹å¸ˆï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
    {question}

    è¯·æä¾›ï¼š
    1. é—®é¢˜åŸå› 
    2. è§£å†³æ­¥éª¤
    3. é¢„é˜²æªæ–½
    """
    return agent.query(enhanced_question)
```

### æ¡ˆä¾‹ 2ï¼šäº§å“çŸ¥è¯†åº“

```python
# product_agent.py
from agents.rag_agent import RAGAgent

agent = RAGAgent()

# åˆ†åˆ«åŠ è½½ä¸åŒäº§å“çº¿
agent.load_documents("./products/series_a")
agent.load_documents("./products/series_b")

def product_query(product_line, question):
    # åœ¨æç¤ºè¯ä¸­æŒ‡å®šäº§å“çº¿
    enhanced_prompt = f"""
    åŸºäº {product_line} äº§å“çš„æ–‡æ¡£å›ç­”ï¼š
    {question}
    """
    return agent.query(enhanced_prompt)
```

### æ¡ˆä¾‹ 3ï¼šå­¦ä¹ åŠ©æ‰‹

```python
# study_agent.py
from agents.rag_agent import RAGAgent
import datetime

agent = RAGAgent()
agent.load_documents("./study_materials")

def study_tutor(question, user_level="åˆçº§"):
    prompt = f"""
    ä½ æ˜¯ä¸€ä½{user_level}æ°´å¹³çš„ç¼–ç¨‹å¯¼å¸ˆã€‚

    å­¦ç”Ÿé—®é¢˜ï¼š{question}

    è¯·ç”¨é€‚åˆ{user_level}å­¦ä¹ è€…çš„è¯­è¨€è§£é‡Šï¼Œ
    å¹¶æä¾›ç›¸å…³ç¤ºä¾‹ä»£ç ã€‚
    """
    return agent.query(prompt)
```

---

## ğŸ“ è¿›é˜¶æŠ€å·§

### 1. é‡æ’åºï¼ˆRerankingï¼‰

```python
# æ£€ç´¢æ›´å¤šæ–‡æ¡£ï¼Œç„¶åé‡æ’åº
docs = vectorstore.similarity_search(question, k=10)

# ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’åº
from sentence_transformers import CrossEncoder
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

reranked_docs = reranker.rank(question, [d.page_content for d in docs])
top_3 = [docs[i] for i, _ in sorted(reranked_docs, key=lambda x: -x[1])[:3]]
```

### 2. æŸ¥è¯¢æ‰©å±•

```python
def query_expansion(question):
    # ä½¿ç”¨ LLM ç”Ÿæˆç›¸ä¼¼é—®é¢˜
    expansion_prompt = f"""
    ç”Ÿæˆ 3 ä¸ªä¸ä»¥ä¸‹é—®é¢˜ç›¸å…³çš„ç­‰ä»·é—®é¢˜ï¼š
    {question}

    åªè¾“å‡ºé—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªã€‚
    """
    related_questions = llm.invoke(expansion_prompt).content.split("\n")

    # åˆå¹¶æ£€ç´¢ç»“æœ
    all_docs = []
    for q in [question] + related_questions:
        all_docs.extend(vectorstore.similarity_search(q, k=2))

    # å»é‡
    return list({d.page_content: d for d in all_docs}.values())
```

### 3. è‡ªå®šä¹‰æ£€ç´¢å™¨

```python
from langchain_core.retrievers import BaseRetriever

class CustomRetriever(BaseRetriever):
    def _get_relevant_documents(self, query, run_manager):
        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        docs = vectorstore.similarity_search(query, k=5)

        # è¿‡æ»¤æ¡ä»¶
        filtered = [d for d in docs if self._is_relevant(d, query)]

        return filtered[:3]

    def _is_relevant(self, doc, query):
        # è‡ªå®šä¹‰ç›¸å…³æ€§åˆ¤æ–­
        score = self._calculate_score(doc, query)
        return score > 0.7
```

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### æ£€ç´¢è´¨é‡è¯„ä¼°

```python
def evaluate_retrieval(questions, ground_truth_docs):
    precision_scores = []
    recall_scores = []

    for q, true_docs in zip(questions, ground_truth_docs):
        retrieved = vectorstore.similarity_search(q, k=3)

        # ç²¾ç¡®ç‡
        precision = len(set(retrieved) & set(true_docs)) / len(retrieved)
        precision_scores.append(precision)

        # å¬å›ç‡
        recall = len(set(retrieved) & set(true_docs)) / len(true_docs)
        recall_scores.append(recall)

    return {
        "avg_precision": sum(precision_scores) / len(precision_scores),
        "avg_recall": sum(recall_scores) / len(recall_scores),
    }
```

---

## ğŸ› ï¸ è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹æ£€ç´¢ç»“æœ

```python
# ä¸´æ—¶ä¿®æ”¹ agent.query() æ–¹æ³•
def debug_query(question):
    # è·å–æ£€ç´¢çš„æ–‡æ¡£
    docs = agent.vectorstore.similarity_search(question, k=3)

    print("=" * 50)
    print("æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š")
    for i, doc in enumerate(docs, 1):
        print(f"\n[æ–‡æ¡£ {i}]")
        print(f"å†…å®¹: {doc.page_content[:100]}...")
        print(f"å…ƒæ•°æ®: {doc.metadata}")
    print("=" * 50)

    # æ­£å¸¸æŸ¥è¯¢
    return agent.query(question)
```

### ç›¸ä¼¼åº¦åˆ†æ•°åˆ†æ

```python
# æŸ¥çœ‹æ¯ä¸ªæ£€ç´¢æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°
docs_with_scores = agent.vectorstore.similarity_search_with_score(
    "ä½ çš„é—®é¢˜", k=5
)

for doc, score in docs_with_scores:
    print(f"åˆ†æ•°: {score:.4f} | {doc.page_content[:50]}")

# å¦‚æœæ‰€æœ‰åˆ†æ•°éƒ½å¾ˆé«˜ï¼ˆ>0.9ï¼‰ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æŸ¥è¯¢
# å¦‚æœåˆ†æ•°éƒ½å¾ˆä½ï¼ˆ<0.5ï¼‰ï¼Œå¯èƒ½çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹
```

---

## ğŸ¯ æ€»ç»“

### RAG æˆåŠŸçš„å…³é”®è¦ç´ 

1. **é«˜è´¨é‡çš„çŸ¥è¯†åº“** - å†…å®¹å‡†ç¡®ã€ç»“æ„æ¸…æ™°
2. **åˆç†çš„åˆ†å—ç­–ç•¥** - æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´
3. **ç²¾å‡†çš„æ£€ç´¢** - ä½¿ç”¨åˆé€‚çš„ K å€¼å’Œç›¸ä¼¼åº¦é˜ˆå€¼
4. **ä¼˜åŒ–çš„æç¤ºè¯** - å¼•å¯¼ AI ç”Ÿæˆæ›´å¥½çš„å›ç­”
5. **æŒç»­çš„è¿­ä»£** - æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´å‚æ•°

### å­¦ä¹ è·¯å¾„

```
åŸºç¡€ï¼ˆå½“å‰ï¼‰ â†’ è¿›é˜¶ â†’ ä¸“å®¶
   â†“           â†“        â†“
ä½¿ç”¨ RAG     é‡æ’åº    å¤šæ¨¡æ€ RAG
è°ƒæ•´å‚æ•°     æŸ¥è¯¢æ‰©å±•   åˆ†å¸ƒå¼ RAG
æµ‹è¯•æ•ˆæœ     æ··åˆæ£€ç´¢   å®æ—¶æ›´æ–°
```

---

**ç¥ä½ æˆä¸º RAG ä¸“å®¶ï¼ğŸ‰**
