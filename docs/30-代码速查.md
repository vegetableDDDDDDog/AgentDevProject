# ğŸ“ ä»£ç é€ŸæŸ¥æ‰‹å†Œ

> å¸¸ç”¨ä»£ç ç‰‡æ®µå¿«é€ŸæŸ¥æ‰¾

---

## ğŸ“‹ ç›®å½•

1. [åŸºç¡€é…ç½®](#åŸºç¡€é…ç½®)
2. [LLM è°ƒç”¨](#llm-è°ƒç”¨)
3. [æç¤ºè¯æ¨¡æ¿](#æç¤ºè¯æ¨¡æ¿)
4. [è®°å¿†ç®¡ç†](#è®°å¿†ç®¡ç†)
5. [å·¥å…·å®šä¹‰](#å·¥å…·å®šä¹‰)
6. [RAG å®ç°](#rag-å®ç°)
7. [æ•°æ®åº“æ“ä½œ](#æ•°æ®åº“æ“ä½œ)
8. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)

---

## ğŸ”§ åŸºç¡€é…ç½®

### ç¯å¢ƒå˜é‡åŠ è½½

```python
from dotenv import load_dotenv
import os

load_dotenv()

API_KEY = os.getenv("OPENAI_API_KEY")
API_BASE = os.getenv("OPENAI_API_BASE")
MODEL = os.getenv("OPENAI_MODEL", "glm-4")
```

### é¡¹ç›®è·¯å¾„å¤„ç†

```python
import os

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(PROJECT_ROOT, "data", "chat_history.db")
```

---

## ğŸ¤– LLM è°ƒç”¨

### åˆ›å»º LLM å®ä¾‹

```python
from langchain_openai import ChatOpenAI

# åŸºç¡€é…ç½®
llm = ChatOpenAI(
    model="glm-4",
    api_key=API_KEY,
    base_url=API_BASE,
    temperature=0.7,
)

# æµå¼è¾“å‡º
async for chunk in llm.astream("ä½ å¥½"):
    print(chunk.content, end="", flush=True)
```

### åŸºç¡€è°ƒç”¨

```python
# ç®€å•è°ƒç”¨
response = llm.invoke("ä»‹ç»ä¸€ä¸‹ Python")
print(response.content)

# å¸¦ç³»ç»Ÿæç¤º
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯ REST APIï¼Ÿ"),
]
response = llm.invoke(messages)
```

### æ‰¹é‡è°ƒç”¨

```python
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
responses = llm.batch(questions)
```

---

## ğŸ“ æç¤ºè¯æ¨¡æ¿

### åŸºç¡€æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}åŠ©æ‰‹"),
    ("human", "{question}"),
])

# ä½¿ç”¨
formatted = prompt.format(
    role="æŠ€æœ¯",
    question="ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"
)
```

### å¸¦å†å²æ¶ˆæ¯

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}"),
])
```

### éƒ¨åˆ†æ ¼å¼åŒ–

```python
# é¢„è®¾éƒ¨åˆ†å˜é‡
prompt = ChatPromptTemplate.from_template(
    "ä½ æ˜¯{role}ï¼Œè¯·å›ç­”ï¼š{question}"
)

partial_prompt = prompt.partial(role="æŠ€æœ¯ä¸“å®¶")

# åç»­åªéœ€æä¾› question
formatted = partial_prompt.format(question="ä»€ä¹ˆæ˜¯ AIï¼Ÿ")
```

### æ¨¡æ¿ç»„åˆ

```python
from langchain_core.prompts import PromptTemplate

# å®šä¹‰å­æ¨¡æ¿
intro = PromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ª{role}ã€‚")
task = PromptTemplate.from_template("è¯·å›ç­”ï¼š{question}")

# ç»„åˆ
full_template = intro + task
formatted = full_template.format(role="åŠ©æ‰‹", question="é—®é¢˜")
```

---

## ğŸ§  è®°å¿†ç®¡ç†

### åŸºç¡€è®°å¿†

```python
from langchain_community.chat_message_histories import ChatMessageHistory

# åˆ›å»ºè®°å¿†
history = ChatMessageHistory()
history.add_user_message("ä½ å¥½")
history.add_ai_message("ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ ï¼Ÿ")

# è·å–æ¶ˆæ¯
messages = history.messages
```

### SQLite æŒä¹…åŒ–

```python
import sqlite3
from langchain_core.messages import HumanMessage, AIMessage

class SQLiteChatMessageHistory:
    def __init__(self, session_id: str, db_path: str):
        self.session_id = session_id
        self.conn = sqlite3.connect(db_path)
        self._init_db()

    def _init_db(self):
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chat_messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                type TEXT,
                content TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()

    def add_messages(self, messages):
        cursor = self.conn.cursor()
        for msg in messages:
            msg_type = "human" if isinstance(msg, HumanMessage) else "ai"
            cursor.execute(
                "INSERT INTO chat_messages (session_id, type, content) VALUES (?, ?, ?)",
                (self.session_id, msg_type, msg.content)
            )
        self.conn.commit()

    @property
    def messages(self):
        cursor = self.conn.cursor()
        cursor.execute(
            "SELECT type, content FROM chat_messages WHERE session_id = ? ORDER BY id",
            (self.session_id,)
        )
        rows = cursor.fetchall()

        messages = []
        for msg_type, content in rows:
            if msg_type == "human":
                messages.append(HumanMessage(content=content))
            else:
                messages.append(AIMessage(content=content))
        return messages
```

### å†å²è£å‰ª

```python
from langchain_core.messages import SystemMessage

MAX_HISTORY = 10

def trim_messages(messages):
    """è£å‰ªå†å²æ¶ˆæ¯ï¼Œä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„ N æ¡"""

    if len(messages) <= MAX_HISTORY:
        return messages

    # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
    system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
    other_msgs = [m for m in messages if not isinstance(m, SystemMessage)]

    # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
    trimmed = system_msgs + other_msgs[-MAX_HISTORY:]
    return trimmed
```

### RunnableWithMessageHistory

```python
from langchain_core.runnables.history import RunnableWithMessageHistory

def get_session_history(session_id):
    return SQLiteChatMessageHistory(session_id, "chat_history.db")

with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# ä½¿ç”¨
response = with_history.invoke(
    {"input": "ä½ å¥½"},
    config={"configurable": {"session_id": "user123"}}
)
```

---

## ğŸ› ï¸ å·¥å…·å®šä¹‰

### åŸºç¡€å·¥å…·

```python
from langchain_core.tools import tool

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2 + 2"

    Returns:
        è®¡ç®—ç»“æœ
    """
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

# è·å–å·¥å…·ä¿¡æ¯
print(calculator.name)      # "calculator"
print(calculator.description)  # å·¥å…·æè¿°
print(calculator.args)      # å‚æ•° schema
```

### å¸¦å…ƒæ•°æ®çš„å·¥å…·

```python
@tool(return_direct=True)
async def search_database(query: str) -> str:
    """æœç´¢æ•°æ®åº“

    Args:
        query: æœç´¢å…³é”®è¯
    """
    # æœç´¢é€»è¾‘
    return f"æœç´¢ç»“æœ"
```

### å·¥å…·åˆ—è¡¨

```python
tools = [
    calculator,
    get_current_time,
    word_counter,
]
```

### ç»‘å®šå·¥å…·åˆ° LLM

```python
# æ–¹æ³• 1: ç›´æ¥ç»‘å®š
llm_with_tools = llm.bind_tools(tools)

# æ–¹æ³• 2: ä½¿ç”¨å·¥å…·è°ƒç”¨ Agent
from langchain.agents import create_tool_calling_agent

agent = create_tool_calling_agent(
    llm,
    tools,
    prompt
)
```

### æ‰§è¡Œå·¥å…·è°ƒç”¨

```python
response = llm_with_tools.invoke("è®¡ç®— 123 * 456")

# æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
if hasattr(response, 'tool_calls') and response.tool_calls:
    for tool_call in response.tool_calls:
        tool_name = tool_call['name']
        tool_args = tool_call['args']

        # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
        for tool in tools:
            if tool.name == tool_name:
                result = tool.invoke(tool_args)
                print(f"ç»“æœ: {result}")
```

---

## ğŸ“š RAG å®ç°

### æ–‡æ¡£åŠ è½½

```python
from langchain_community.document_loaders import TextLoader, DirectoryLoader

# åŠ è½½å•ä¸ªæ–‡ä»¶
loader = TextLoader("file.txt", autodetect_encoding=True)
documents = loader.load()

# åŠ è½½æ•´ä¸ªç›®å½•
loader = DirectoryLoader(
    "./docs",
    glob="**/*.txt",
    loader_cls=TextLoader,
    show_progress=True,
)
documents = loader.load()
```

### æ–‡æœ¬åˆ†å‰²

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""],
)

splits = text_splitter.split_documents(documents)
```

### åˆ›å»ºå‘é‡å­˜å‚¨

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="embedding-3",
    api_key=API_KEY,
    base_url=API_BASE,
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db",
)
```

### ç›¸ä¼¼åº¦æ£€ç´¢

```python
# åŸºç¡€æ£€ç´¢
docs = vectorstore.similarity_search("é—®é¢˜", k=3)

# å¸¦åˆ†æ•°çš„æ£€ç´¢
docs_with_scores = vectorstore.similarity_search_with_score("é—®é¢˜", k=3)
for doc, score in docs_with_scores:
    print(f"ç›¸ä¼¼åº¦: {score:.4f} | {doc.page_content[:50]}")

# æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ£€ç´¢ï¼ˆMMRï¼‰
docs = vectorstore.max_marginal_relevance_search("é—®é¢˜", k=3, fetch_k=10)
```

### RAG Chain

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}
""")

def retrieve_context(inputs):
    docs = vectorstore.similarity_search(inputs["question"], k=3)
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {
        "context": retrieve_context,
        "question": lambda x: x["question"],
    }
    | prompt
    | llm
    | StrOutputParser()
)

# ä½¿ç”¨
answer = rag_chain.invoke({"question": "ä½ çš„é—®é¢˜"})
```

---

## ğŸ’¾ æ•°æ®åº“æ“ä½œ

### SQLite åŸºç¡€

```python
import sqlite3

# è¿æ¥æ•°æ®åº“
conn = sqlite3.connect("chat_history.db")
cursor = conn.cursor()

# åˆ›å»ºè¡¨
cursor.execute("""
    CREATE TABLE IF NOT EXISTS messages (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        session_id TEXT,
        content TEXT,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    )
""")
conn.commit()

# æ’å…¥æ•°æ®
cursor.execute(
    "INSERT INTO messages (session_id, content) VALUES (?, ?)",
    ("user123", "Hello")
)
conn.commit()

# æŸ¥è¯¢æ•°æ®
cursor.execute(
    "SELECT content, timestamp FROM messages WHERE session_id = ?",
    ("user123",)
)
rows = cursor.fetchall()

# å…³é—­è¿æ¥
conn.close()
```

### ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
from contextlib import contextmanager

@contextmanager
def get_db_connection(db_path):
    conn = sqlite3.connect(db_path)
    try:
        yield conn
    finally:
        conn.close()

# ä½¿ç”¨
with get_db_connection("chat_history.db") as conn:
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM messages")
    # è‡ªåŠ¨å…³é—­è¿æ¥
```

---

## âš ï¸ é”™è¯¯å¤„ç†

### åŸºç¡€å¼‚å¸¸å¤„ç†

```python
def safe_query(question: str) -> str:
    try:
        response = agent.query(question)
        return response
    except AttributeError as e:
        return f"âš ï¸ å·¥å…·è°ƒç”¨é”™è¯¯: {e}"
    except Exception as e:
        return f"âŒ æœªçŸ¥é”™è¯¯: {e}"
```

### é‡è¯•æœºåˆ¶

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def llm_call_with_retry(prompt):
    return llm.invoke(prompt)
```

### è¶…æ—¶æ§åˆ¶

```python
import signal
from contextlib import contextmanager

@contextmanager
def timeout(seconds):
    def timeout_handler(signum, frame):
        raise TimeoutError("æ“ä½œè¶…æ—¶")

    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)

# ä½¿ç”¨
try:
    with timeout(30):
        response = llm.invoke("å¾ˆé•¿çš„æç¤ºè¯...")
except TimeoutError:
    print("LLM è°ƒç”¨è¶…æ—¶")
```

### é™çº§ç­–ç•¥

```python
def agent_with_fallback(question):
    try:
        # å°è¯•ä½¿ç”¨å®Œæ•´åŠŸèƒ½
        return rag_agent.query(question)
    except Exception as e:
        print(f"RAG å¤±è´¥: {e}ï¼Œé™çº§åˆ°åŸºç¡€ LLM")
        # é™çº§åˆ°åŸºç¡€ LLM
        return llm.invoke(question)
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹

### æœ€å° Agent

```python
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="glm-4")
response = llm.invoke("ä½ å¥½ï¼")
print(response.content)
```

### å®Œæ•´ RAG

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. åˆå§‹åŒ–
llm = ChatOpenAI(model="glm-4")
embeddings = OpenAIEmbeddings(model="embedding-3")

# 2. åŠ è½½æ–‡æ¡£
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
)

# 3. åˆ›å»º Chain
prompt = ChatPromptTemplate.from_template("""
{context}

é—®é¢˜ï¼š{question}
""")

def retriever(inputs):
    docs = vectorstore.similarity_search(inputs["question"], k=3)
    return {"context": "\n".join(d.page_content for d in docs), **inputs}

chain = retriever | prompt | llm | StrOutputParser()

# 4. æŸ¥è¯¢
answer = chain.invoke({"question": "ä½ çš„é—®é¢˜"})
```

---

## ğŸ“š æ›´å¤šèµ„æº

- ğŸ“– [æœ€ä½³å®è·µ](20-æœ€ä½³å®è·µ.md)
- ğŸ”§ [å¸¸è§é—®é¢˜](31-å¸¸è§é—®é¢˜.md)
- ğŸ“ [å­¦ä¹ è·¯çº¿å›¾](32-å­¦ä¹ è·¯çº¿å›¾.md)

---

**é€ŸæŸ¥æ‰‹å†ŒæŒç»­æ›´æ–°ä¸­...** ğŸ“
